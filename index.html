<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Kartik Narayan</title>

  <meta name="author" content="Kartik Narayan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/face-icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:left">
                    <name>Kartik Narayan</name>
                  </p>
                  <p>I am a 2nd year Ph.D. student in the <a href="https://www.cs.jhu.edu">Computer Science
                      department</a>
                    at <a href="https://www.jhu.edu">Johns Hopkins University</a>, where I am a member of <a
                      href="https://engineering.jhu.edu/vpatel36/">VIU lab</a>, advised by <a
                      href="https://engineering.jhu.edu/faculty/vishal-patel/">Dr.Vishal Patel</a>. My research focuses
                    on computer vision and its applications in face analysis, understanding, and recognition, with a
                    particular emphasis on multimodal LLMs and unified vision models.
                  </p>
                  <p>
                    Prior to my doctoral studies, I worked as an undergraduate researcher under
                    <a href="http://home.iitj.ac.in/~richa/">Prof. Richa Singh</a> and
                    <a href="http://home.iitj.ac.in/~mvatsa/">Prof. Mayank Vatsa</a> at the
                    <a href="http://iab-rubric.org">Image Analysis and Biometrics (IAB) Lab</a>,
                    IIT Jodhpur, where I worked on deepfake video generation. During my time at IIT Jodhpur,
                    I also collaborated with
                    <a href="http://home.iitj.ac.in/~suman/">Dr. Suman Kundu</a> and
                    <a href="https://sites.google.com/site/suchetana0116">Dr. Suchetana Chakraborty</a>.
                    Additionally, I interned at the University of Texas, San Antonio, where I worked with
                    <a href="https://drheenarathore.wordpress.com">Dr. Heena Rathore</a> and
                    <a href="https://scholar.google.com/citations?user=w_LvvecAAAAJ&hl=en">Dr. Faycal Znidi</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:kartiknarayan1@gmail.com">Email</a> &nbsp/&nbsp
                    <a href="data/Kartik_Resume.pdf">CV</a> &nbsp/&nbsp
                    <!-- <a href="data/KartikNarayan_bio.txt">Bio</a> &nbsp/&nbsp -->
                    <a href="https://scholar.google.com/citations?user=bReNhqAAAAAJ&hl=en&oi=ao">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/kartik-narayan-323584199/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://twitter.com/KartikNarayan10">Twitter</a> &nbsp/&nbsp
                    <a href="https://github.com/Kartik-3004">Github</a>
                  </p>
                  <p style="text-align:left; font-weight:bold; color:#d9534f;">
                    Open to internship opportunities for Summer 2025
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/KartikNarayan.jpeg"><img style="width:100%;max-width:100%" alt="profile photo"
                      src="images/KartikNarayan_circle.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">

                  <heading>Research</heading>
                  <p>
                    My research primarily focuses on computer vision and its applications in face analysis,
                    understanding, and recognition,
                    with the goal of developing robust open-world algorithms that can be deployed for real-world impact.
                    My specific
                    research interests include <b style="color:#0000FF;">multimodal LLMs</b>, <b
                      style="color:#0000FF;">unified vision models</b>, parameter-efficient
                    fine-tuning,
                    face segmentation,
                    and representation learning. While most of my work has been in the application domain of facial
                    analysis, I am open to
                    exploring general vision research problems, particularly in the field of multimodal LLMs. I have
                    authored multiple
                    first-author papers with publications in top conferences like CVPR.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>News</heading>
                  <p>
                  <div style="width:100%;overflow-y:scroll; height:160px;">
                    <ul>
                      <li><b>October, 2024:</b> One Paper is accepted at <b>WACV 2025.</b> </li>
                      <li><b>October, 2024:</b> One Paper is accepted at <b>IEEE TBIOM.</b> </li>
                      <li><b>January, 2024:</b> One Paper is accepted at <b>FG 2024.</b> </li>
                      <li><b>August, 2023:</b> Joined as a PhD student at <b>VIU Lab, Johns Hopkins University.</b>
                      </li>
                      <li><b>May, 2023:</b> Received <b>CVPR Student Travel Award.</b> </li>
                      <li><b>Feb, 2023:</b> One Paper is Accepted at <b>CVPR 2023.</b></li>
                      <li><b>September, 2022:</b> One Paper is Accepted at <b>IEEE Access.</b> </li>
                      <li><b>August, 2022:</b> One Paper is Accepted at <b>IJCB 2022.</b></li>
                      <li><b>April, 2022:</b> One Paper is Accepted at <b>CVPRW TCV 2022.</b></li>
                      <li> <b>January, 2022:</b> One Paper is Accepted at <b>IEEE SysCon 2022</b>. </li>
                    </ul>
                  </div>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <hr>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Publications</heading>
                  <p>
                    See my <a href="https://scholar.google.com/citations?user=bReNhqAAAAAJ&hl=en&oi=ao">Google
                      Scholar</a> profile for the complete and most recent publications.
                    <!-- Representative papers are <span class="highlight">highlighted</span>. -->
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr bgcolor="#ffffe6">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/dfplatter.jpeg' width="160" , height="100">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/html/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.html">
                    <papertitle>DF-Platter: Multi-subject Heterogeneous Deepfake Dataset
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Harsh Agarwal,
                  Kartik Thakral,
                  Surbhi Mittal,
                  Mayank Vatsa,
                  Richa Singh
                  <br>
                  <br>
                  <em>CVPR </em>, 2023
                  <br>
                  <!-- <a href="https://yifanjiang.net/MalleConv.html">project page</a> -->
                  <!-- / -->
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Narayan_DF-Platter_Multi-Face_Heterogeneous_Deepfake_Dataset_CVPR_2023_paper.pdf">paper</a>
                  /
                  <a href="data/DF-Platter_Poster.pdf">poster</a>
                  <p></p>
                  <p>
                    In this research, we emulate the real-world scenario of deepfake generation and spreading, and
                    propose the DF-Platter dataset which contains (i) both low-resolution and high-resolution deepfakes
                    generated using multiple generation techniques, (ii) single-subject and multiple-subject deepfakes.
                    The results demonstrate a significant performance reduction in the deepfake detection task on
                    low-resolution deepfakes and show that the existing techniques fail drastically on multiple-subject
                    deepfakes.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/facexbench.png' width="160" , height="100">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="papers/facexbench.pdf">
                    <papertitle>FaceXBench: Evaluating Multimodal LLMs on Face Understanding
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Vibashan VS,
                  Vishal M. Patel
                  <br>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="papers/facexbench.pdf">paper</a>
                  <p></p>
                  <p>
                    We introduce FaceXBench, a comprehensive benchmark designed to evaluate MLLMs on complex face
                    understanding tasks. FaceXBench includes 5,000 multimodal multiple-choice questions derived from 25
                    public datasets
                    and a newly created dataset, FaceXAPI. These questions cover 14 tasks across 6 broad categories,
                    assessing MLLMs'
                    face understanding abilities in bias and fairness, face authentication, recognition, analysis,
                    localization and tool
                    retrieval.
                    Using FaceXBench, we conduct an extensive evaluation of 26 open-source MLLMs alongside 2
                    proprietary models,
                    revealing the unique challenges in complex face understanding tasks.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/facex_archi.png' width="160" , height="100">
                  </div>
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2403.12960">
                    <papertitle><i>FaceXFormer</i>: A Unified Transformer for Facial Analysis
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Vibashan VS,
                  Rama Chellappa,
                  Vishal M. Patel
                  <br>
                  <br>
                  <a href="https://arxiv.org/pdf/2403.12960">arXiv</a> /
                  <a href="https://kartik-3004.github.io/facexformer_web/">project</a> /
                  <a href="https://github.com/Kartik-3004/facexformer">code (185 &#x2B50;)</a>
                  <p></p>
                  <p>
                    <i>FaceXFormer</i> is an end-to-end unified model capable of handling a comprehensive range of
                    facial
                    analysis tasks such as
                    face parsing, landmark detection, head pose estimation, attributes recognition, and estimation of
                    age,
                    gender, race, and
                    landmarks visibility. It leverages a transformer-based encoder-decoder architecture where each task
                    is
                    treated as a learnable token, enabling
                    the integration of multiple tasks within a single framework.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/segface.png' width="160" , height="100">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="papers/SegFace.pdf">
                    <papertitle><i>SegFace</i>: Face Segmentation of Long-Tail Classes
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Vibashan VS,
                  Vishal M. Patel
                  <br>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <a href="papers/SegFace.pdf">paper</a>
                  <p></p>
                  <p>
                    We propose <i>SegFace</i>, a simple and efficient transformer-based model which utilizes
                    learnable class-specific tokens, allowing each token to
                    focus on its corresponding class, thereby enabling independent modeling of each class. It improves
                    the performance of long-tail classes and outperforms previous state-of-the-art models, achieving a
                    mean F1 score of 88.96 (+2.82) on the
                    CelebAMask-HQ dataset and 93.03 (+0.65) on the LaPa dataset.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/petalface_intro.png' width="160" , height="140">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="papers/PETALface.pdf">
                    <papertitle>PETAL<i>face</i>: Parameter Efficient Transfer Learning for Low-resolution Face
                      Recognition
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Nithin Gopalkrishnan Nair,
                  Jennifer Xu,
                  Rama Chellappa,
                  Vishal M. Patel
                  <br>
                  <br>
                  <em>WACV 2025</em>
                  <br>
                  <a href="papers/PETALface.pdf">paper</a>
                  <p></p>
                  <p>
                    The proposed PETAL<i>face</i> a parameter efficient transfer learning approach adapts to
                    low-resolution datasets beating the
                    performance of pre-trained models with negligible drop in performance on high-resolution and
                    mixed-quality datasets.
                    PETAL<i>face</i> enables development of generalized models achieving competitive performance on
                    high-resolution (LFW, CFP-FP,
                    CPLFW, AgeDB, CALFW, CFP-FF) and mixed-quality datasets (IJB-B, IJB-C) with big enhancements in
                    low-quality surveillance
                    quality datasets (TinyFace, BRIAR, IJB-S).
                  </p>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/hypoc_archi.png' width="160" , height="100">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2404.14406">
                    <papertitle>Hyp-OC: Hyperbolic One Class Classification for Face Anti-Spoofing
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Vishal M. Patel
                  <br>
                  <br>
                  <em>IEEE International Conference on Automatic Face and Gesture Recognition (FG)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2404.14406">paper</a> /
                  <a href="https://kartik-3004.github.io/hyp-oc/">project</a> /
                  <a href="https://github.com/Kartik-3004/hyp-oc">code</a>
                  <p></p>
                  <p>
                    Most prior research in face anti-spoofing (FAS) approaches it as a two-class classification task
                    where models are
                    trained on real samples and known spoof attacks and tested for detection performance on unknown
                    spoof attacks. However,
                    in practice, FAS should be treated as a one-class classification task where, while training, one
                    cannot assume any
                    knowledge regarding the spoof samples a priori. Hyp-OC, is the first work exploring hyperbolic
                    embeddings for one-class
                    face anti-spoofing (OC-FAS).
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/briar.png' width="160" , height="80">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Improved Representation Learning for Unconstrained Face Recognition
                  </papertitle>
                  <br>
                  Nithin Gopalkrishnan Nair,
                  <strong>Kartik Narayan</strong>,
                  Maitreya Suin,
                  Ram Prabhakar,
                  Jennifer Xu,
                  Soraya Stevens,
                  Joshua Gleason,
                  Nathan Shnidman,
                  Rama Chellappa,
                  Vishal M. Patel
                  <br>
                  <br>
                  <em>Under Review</em>
                  <br>
                  <p></p>
                  <p>
                    In this
                    work, we tackle the problem of low-quality face recognition and take a closer look at the nature of
                    data
                    in low resolution datasets and redefine paradigms in terms of model choice, data input pipeline and
                    fine-tuning schemes.
                    With the accumulated effect of all our design choices, we achieve state-of-the-art results in
                    mixed-quality benchmarks
                    (IJB-B, IJB-C) as well as multiple challenging benchmarks for unconstrained face recognition
                    (Tinyface, IJB-S and
                    BRIAR), thereby opening up a new avenue of research in the area.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/deephynet.png' width="160" , height="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="papers/DeePhyNet.pdf">
                    <papertitle>DeePhyNet: Towards Detecting Phylogeny in Deepfakes
                    </papertitle>
                  </a>
                  <br>
                  Kartik Thakral,
                  Harsh Agarwal,
                  <strong>Kartik Narayan</strong>,
                  Surbhi Mittal,
                  Mayank Vatsa,
                  Richa Singh
                  <br>
                  <br>
                  <em>Accepted in TBIOM</em>
                  <br>
                  <a href="papers/DeePhyNet.pdf">paper</a>
                  <p></p>
                  <p>
                    We propose DeePhyNet, which performs three tasks: it first differentiates between real and fake
                    content; it next
                    determines the signature of the generative algorithm used for deepfake creation to determine which
                    algorithm has been
                    used for generation, and finally, it also predicts the phylogeny of algorithms used for generation.
                    To the best of our
                    knowledge, this is the first algorithm that performs all three tasks together for deepfake media
                    analysis.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/ijcb.jpg' width="160" , height="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/10007968/">
                    <papertitle>DeePhy: On Deepfake Phylogeny
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Harsh Agarwal,
                  Kartik Thakral,
                  Surbhi Mittal,
                  Mayank Vatsa,
                  Richa Singh
                  <br>
                  <br>
                  <em>International Joint Conferece on Biometrics (IJCB)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2209.09111">paper</a>
                  <p></p>
                  <p>
                    We proposed the idea of DeepFake Phylogeny and a complementary dataset DeePhy. The paper shows the
                    need to evolve the research of model attribution of deepfakes and facilitates advancements in real
                    life scenarios of plagiarism detection, forgery detection, and reverse engineering of deepfakes.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/desi.jpg' width="160" , height="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/html/Narayan_DeSI_Deepfake_Source_Identifier_for_Social_Media_CVPRW_2022_paper.html">
                    <papertitle>DeSI: Deepfake Source Identifier for Social Media
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Harsh Agarwal,
                  Surbhi Mittal,
                  Kartik Thakral,
                  Suman Kundu,
                  Mayank Vatsa,
                  Richa Singh
                  <br>
                  <br>
                  <em>CVPR Workshops</em>, 2022
                  <br>
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022W/FaDE-TCV/papers/Narayan_DeSI_Deepfake_Source_Identifier_for_Social_Media_CVPRW_2022_paper.pdf">paper</a>
                  <p></p>
                  <p>
                    We develop an algorithm to find the source/propagator of tweets with deepfake/manipulated
                    images/videos relevant to a given text query. The result is shown in form of a force-directed graph
                    which gives temporal insight into the spread pattern and also identifies the volatile nodes in the
                    network by predicting the virality of tweets.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/access.jpg' width="160" , height="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9893130">
                    <papertitle>Using Epidemic Modelling, Machine Learning and Control Feedback Strategy for Policy
                      Management of COVID-19
                    </papertitle>
                  </a>
                  <br>
                  <strong>Kartik Narayan</strong>,
                  Heena Rathore,
                  Faycal Znidi
                  <br>
                  <br>
                  <em>IEEE Access</em>, 2022
                  <br>
                  <a href="papers/Access.pdf">paper</a>
                  /
                  <a href="https://github.com/Kartik-3004/SIR_pandemic">code</a>
                  <p></p>
                  <p>
                    We propose a threshold mechanism for policy control by analyzing the SIR model and estimating the
                    optimal parameters. Our work helps keep the economic impact of a pandemic under control and also
                    helps in predicting the approximate duration of the lockdwon.
                  </p>
                </td>
              </tr>


              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/syscon_2.jpg' width="160" , height="160">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9773844">
                    <papertitle>Leveraging ambient sensing for the estimation of curiosity-driven human crowd
                    </papertitle>
                  </a>
                  <br>
                  Anirban Das,
                  <strong>Kartik Narayan</strong>,
                  Suchetana Chakraborty
                  <br>
                  <br>
                  <em>IEEE Systems Conference (SysCon)</em>, 2022
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9773844">paper</a>
                  <p></p>
                  <p>
                    We predicted the curious crowd attracted to an object by measuring it's spatiotemporal significance.
                    The work utilizes a set of passive sensors and wireless signal properties for the estimation.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" ;align="center" ;border="0px" ;cellspacing="0" ;cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://jonbarron.info/">Template credits</a>. Last updated October 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>


</body>

</html>